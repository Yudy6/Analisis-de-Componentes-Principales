---
title: "An√°lisis de Componentes Principales"
output: html_document
date: "2025-11-25"
editor_options: 
  markdown: 
    wrap: 72
---

# 4.1.1: Las componentes principales como direcciones en las que la matriz de datos tiene la m√°xima varianza

------------------------------------------------------------------------

## Contexto inicial y preparaci√≥n de datos

Partimos de una matriz de datos original:

```{r}
datos_filtrados <- datos_filtrados
# ANTES: Verificar cantidad de NA por variable
cat("=== CANTIDAD DE NA POR VARIABLE (ANTES) ===\n")
na_antes <- sapply(datos_filtrados, function(x) sum(is.na(x)))
print(na_antes)

```

$$
X_{n \times p} = \{x_{ij}\}
$$ donde: - $n$: n√∫mero de objetos (individuos, ciudades, etc.) - $p$:
n√∫mero de variables - $x_{ij}$: valor de la variable $j$ en el objeto
$i$

### Estad√≠sticos b√°sicos por variable:

-   Media: $\bar{x}_j = \frac{1}{n}\sum_{i=1}^n x_{ij}$
-   Varianza: $s_j^2 = \frac{1}{n}\sum_{i=1}^n (x_{ij} - \bar{x}_j)^2$

### **Matriz de datos centrados y estandarizados:**

$$
Y_{n \times p} = \{y_{ij}\} = \left\{\frac{x_{ij} - \bar{x}_j}{s_j}\right\}
$$

**Propiedades de Y:** - Media cero:
$\bar{Y}_j = \frac{1}{n}\sum_{i=1}^n y_{ij} = 0$ - Varianza unitaria:
$var(Y_j) = \frac{1}{n}\sum_{i=1}^n y_{ij}^2 = 1$

------------------------------------------------------------------------

## Representaci√≥n de la matriz Y

Podemos ver $Y$ de dos formas:

1.  **Como vectores fila:** $y_i' = (y_{i1}, y_{i2}, \ldots, y_{ip})$
    para $i = 1,\ldots,n$\
    ‚Üí An√°lisis de similitudes entre objetos

2.  **Como vectores columna:** $Y_j = (y_{1j}, y_{2j}, \ldots, y_{nj})'$
    para $j = 1,\ldots,p$\
    ‚Üí An√°lisis de asociaciones entre variables

------------------------------------------------------------------------

## Primera componente principal: direcci√≥n de m√°xima varianza

### **Objetivo:**

Encontrar un vector de ponderaciones $u_1' = (u_{11}, \ldots, u_{p1})$
normalizado ($u_1'u_1 = 1$) que maximice la varianza de la combinaci√≥n
lineal: $$
z_{i1} = u_1'y_i = \sum_{j=1}^p u_{j1}y_{ij}
$$

###Formulaci√≥n del problema de optimizaci√≥n:

La varianza de $z_1$ es: $$
var(z_1) = \frac{1}{n}\sum_{i=1}^n z_{i1}^2 = \frac{1}{n}\sum_{i=1}^n \left(\sum_{j=1}^p u_{j1}y_{ij}\right)^2
$$

**Problema de optimizaci√≥n:** $$
\max_{u_{11},\ldots,u_{p1}} \left\{ \frac{1}{n}\sum_{i=1}^n \left(\sum_{j=1}^p u_{j1}y_{ij}\right)^2 \right\}
$$ sujeto a: $\sum_{j=1}^p u_{j1}^2 = 1$

------------------------------------------------------------------------

## Resoluci√≥n mediante multiplicadores de Lagrange

### Funci√≥n de Lagrange:

$$
\mathcal{L}(u_1, \lambda) = u_1'Ru_1 - \lambda(u_1'u_1 - 1)
$$ donde $R = \frac{1}{n}Y'Y$ es la **matriz de correlaci√≥n** (cuando
trabajamos con datos estandarizados).

### Derivadas e igualaci√≥n a cero:

1.  **Derivada respecto a** $u_1$: $$
    \frac{\partial \mathcal{L}}{\partial u_1} = 2Ru_1 - 2\lambda u_1 = 0
    $$ $$
    \Rightarrow Ru_1 = \lambda u_1
    $$

2.  **Derivada respecto a** $\lambda$: $$
    \frac{\partial \mathcal{L}}{\partial \lambda} = u_1'u_1 - 1 = 0
    $$ $$
    \Rightarrow u_1'u_1 = 1
    $$

### Interpretaci√≥n del resultado:

La ecuaci√≥n $Ru_1 = \lambda u_1$ nos dice que: - $u_1$ es un **vector
propio** de la matriz $R$ - $\lambda$ es el **valor propio**
correspondiente

Para **maximizar la varianza**, elegimos el **mayor valor propio**
$\lambda_1$ de $R$, y su vector propio correspondiente $u_1$.

------------------------------------------------------------------------

## Segunda componente principal

### **Objetivo:**

Encontrar un segundo vector $u_2' = (u_{12}, \ldots, u_{p2})$ que: - Sea
ortogonal a $u_1$: $u_1'u_2 = 0$ - Est√© normalizado: $u_2'u_2 = 1$ -
Maximice la varianza de $z_2 = Yu_2$

### **Problema de optimizaci√≥n:**

$$
\max_{u_2} \left\{ \frac{1}{n}\sum_{i=1}^n \left(\sum_{j=1}^p u_{j2}y_{ij}\right)^2 \right\}
$$ sujeto a: - $u_2'u_2 = 1$ - $u_1'u_2 = 0$

### **Funci√≥n de Lagrange ampliada:**

$$
\mathcal{L}(u_2, \lambda_2, \gamma) = u_2'Ru_2 - \lambda_2(u_2'u_2 - 1) - \gamma u_1'u_2
$$

### **Derivadas:**

$$
\frac{\partial \mathcal{L}}{\partial u_2} = 2Ru_2 - 2\lambda_2 u_2 - \gamma u_1 = 0
$$ $$
\frac{\partial \mathcal{L}}{\partial \lambda_2} = u_2'u_2 - 1 = 0
$$ $$
\frac{\partial \mathcal{L}}{\partial \gamma} = u_1'u_2 = 0
$$

### **Demostraci√≥n de que** $\gamma = 0$:

Multiplicamos la primera ecuaci√≥n por $u_1'$ por la izquierda: $$
2u_1'Ru_2 - 2\lambda_2 u_1'u_2 - \gamma u_1'u_1 = 0
$$

Sabemos que: - $u_1'u_2 = 0$ (restricci√≥n de ortogonalidad) -
$u_1'u_1 = 1$ (normalizaci√≥n) -
$u_1'Ru_2 = u_1'(\lambda_1 u_1)'u_2 = \lambda_1 u_1'u_2 = 0$

Por tanto: $$
0 - 0 - \gamma(1) = 0 \Rightarrow \gamma = 0
$$

### **Soluci√≥n:**

Con $\gamma = 0$, la ecuaci√≥n se reduce a: $$
Ru_2 = \lambda_2 u_2
$$

Nuevamente, $u_2$ es un **vector propio** de $R$, y para maximizar la
varianza elegimos el **segundo mayor valor propio** $\lambda_2$.

------------------------------------------------------------------------

## Componentes principales generales

### **Para el Œ±-√©simo componente:**

$$
z_{i\alpha} = \sum_{j=1}^p u_{j\alpha}y_{ij} = y_i'u_\alpha
$$ $$
z_\alpha = Yu_\alpha
$$ donde $u_\alpha$ es el Œ±-√©simo vector propio de $R$ asociado al
Œ±-√©simo mayor valor propio $\lambda_\alpha$.

### **Propiedades:**

1.  **Media cero:** $m(z_\alpha) = 0$
2.  **Varianza:** $var(z_\alpha) = \lambda_\alpha$
3.  **Ortogonalidad:** $z_\alpha' z_\beta = 0$ para $\alpha \neq \beta$

------------------------------------------------------------------------

## Interpretaci√≥n geom√©trica

Cada componente principal $z_\alpha$ representa la **proyecci√≥n** de los
datos originales sobre la direcci√≥n definida por el vector propio
$u_\alpha$.

La **varianza explicada** por cada componente es exactamente igual al
valor propio correspondiente: $$
var(z_\alpha) = \lambda_\alpha
$$

El **porcentaje de varianza explicada** por la Œ±-√©sima componente es: $$
\tau_\alpha = \frac{\lambda_\alpha}{\sum_{\alpha=1}^p \lambda_\alpha}
$$

------------------------------------------------------------------------

## Resumen del procedimiento

1.  **Centrar y estandarizar** los datos:
    $Y = \left\{\frac{x_{ij}-\bar{x}_j}{s_j}\right\}$
2.  **Calcular matriz de correlaci√≥n:** $R = \frac{1}{n}Y'Y$
3.  **Descomposici√≥n espectral:** Encontrar valores y vectores propios
    de $R$
4.  **Ordenar** valores propios en forma descendente:
    $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_p$
5.  **Componentes principales:** $z_\alpha = Yu_\alpha$, donde
    $u_\alpha$ es el Œ±-√©simo vector propio
6.  **Seleccionar** las primeras $q$ componentes que acumulen suficiente
    varianza

------------------------------------------------------------------------

Esta aproximaci√≥n del ACP busca **direcciones de m√°xima varianza** en
los datos, proporcionando una base ortogonal √≥ptima para representar la
informaci√≥n contenida en la matriz original con dimensionalidad
reducida.

------------------------------------------------------------------------

# 4.1.2 Las componentes principales como aproximaci√≥n por m√≠nimos cuadrados de la matriz de datos

------------------------------------------------------------------------
A diferencia del enfoque de la secci√≥n anterior donde las componentes principales se introdujeron como las direcciones en las que los datos presentan la m√°xima variabilidad aqu√≠ se presenta una perspectiva equivalente pero basada en aproximar la matriz de datos mediante un modelo de dimensiones reducidas, utilizando el criterio de m√≠nimos cuadrados.

El punto de partida es nuevamente la matriz estandarizada:


$$
Y_{n \times p} = \{y_{ij}\} = \left\{\frac{x_{ij} - \bar{x}_j}{s_j}\right\}
$$

El objetivo es representar a $ùëå$ mediante una versi√≥n ‚Äúsimplificada‚Äù, es decir, utilizando solo unas pocas componentes principales. Esta idea se formaliza construyendo una aproximaci√≥n de rango reducido de la forma:

$$
\tilde{\mathbf{Y}}^{(q)} = \mathbf{Z}_q \mathbf{U}'_q
$$

donde:

- $\mathbf{U}_q$ es la matriz formada por los primeros $q$ vectores propios de $\mathbf{R}$, de dimensi√≥n $p \times q$.
 
- $\mathbf{Z}_q = \mathbf{Y}\mathbf{U}_q$ contiene los scores o componentes principales, de tama√±o $n \times q$.

- $q < p$ es el n√∫mero de dimensiones deseadas.


Esta representaci√≥n indica que cualquier fila de $\mathbf{Y}$, originalmente un vector en $\mathbb{R}^p$, se aproxima mediante una combinaci√≥n lineal de solo $q$ direcciones ortogonales. 
------------------------------------------------------------------------
##Componente principal
Entonces igual que antes se sabe que una componente es una combinaci√≥n lineal: 
$$
Z_1 = u_{11}X_1 + u_{21}X_2 + \dots + u_{p1}X_p
$$

con 
$$
\sum_{j=1}^p u_{j1}^2 = 1
$$

------------------------------------------------------------------------
##Primera componente 

$$
z_{i1} = \mathbf{u}'_1 \mathbf{y}_i = \sum_{j=1}^p u_{j1} y_{ij}
$$

- $y_{ij}$: datos centrados y estandarizados.
- $\mathbf{u}_1$: vector de pesos (vector propio).
- $z_{i1}$: proyecci√≥n de cada individuo sobre la direcci√≥n de la primera componente.


Como cada columna de $\mathbf{Y}$ tiene media $0$:
- la componente tambi√©n tiene media cero.
- Su varianza es:
$$
\text{var}(\mathbf{z}_1) = \frac{1}{n} \sum_{i=1}^n z_{i1}^2
$$

------------------------------------------------------------------------
##Problema de optimizaci√≥n 

$$
\max_{\mathbf{u}_1} \frac{1}{n} \sum_{i=1}^n \left( \sum_{j=1}^p u_{j1} y_{ij} \right)^2 \quad \text{sujeto a} \quad \sum_{j=1}^p u_{j1}^2 = 1
$$
------------------------------------------------------------------------
##Interpretaci√≥n geom√©trica 
En este caso se tiene que La primera componente es la recta que minimiza las distancias cuadr√°ticas a los puntos. En donde esa recta pasa por el origen y tiene direcci√≥n $u_1$.

Tambien se puede decir que la raz√≥n por la que esta recta es √≥ptima es que minimiza la suma de las distancias cuadr√°ticas entre cada punto original y su proyecci√≥n sobre la recta. Lo cual es equivalente a la definici√≥n basada en maximizar la varianza explicada.

![Interpretaci√≥n geometrica de una componente principal](D:/imagen_1.png)

Si $z_{i1}$ es la coordenada del dato $\mathbf{y}_i$ sobre la primera componente, entonces su proyecci√≥n sobre la recta es:
$$
\hat{\mathbf{y}}_i = z_{i1} \mathbf{u}_1
$$
Esta proyecci√≥n es la mejor aproximaci√≥n posible del punto original usando solo una dimensi√≥n.
------------------------------------------------------------------------
##Aproximaci√≥n usando q componentes


------------------------------------------------------------------------