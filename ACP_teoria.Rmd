---
title: "Análisis de Componentes Principales"
output: html_document
date: "2025-11-25"
editor_options: 
  markdown: 
    wrap: 72
---

En la investigación aplicada, especialmente en ciencias sociales,
biología, economía e ingeniería es común trabajar con conjuntos de datos
de alta dimensionalidad. Aunque un mayor número de variables puede
aportar información valiosa, su análisis simultáneo presenta
dificultades como el aumento de parámetros a estimar, el riesgo de
multicolinealidad y la complejidad para interpretar la estructura
subyacente de los datos. Esto hace necesario emplear métodos que resuman
la información esencial sin perder las características relevantes del
conjunto original.

El Análisis de Componentes Principales (ACP) es una técnica
multivariante diseñada precisamente para reducir la dimensionalidad.
Transforma las variables originales en un nuevo conjunto menor de
variables no correlacionadas, denominadas componentes principales. Estas
componentes son combinaciones lineales de las variables originales,
construidas de modo que capturen la máxima varianza posible,
concentrando así la mayor cantidad de información.

## Definición:

El Análisis de Componentes Principales (ACP) es una técnica estadística
multivariante cuyo objetivo es transformar un conjunto de variables
originales $X_1, X_2, \dots, X_p$ en un nuevo conjunto de variables no
correlacionadas llamadas **componentes principales**. Estas nuevas
variables son combinaciones lineales de las variables originales y se
construyen de forma que capturen la máxima varianza posible, es decir,
la mayor cantidad de información contenida en los datos.

Formalmente, el $k$-ésimo componente principal se define como:

$$
Y_k = a_{1k}X_1 + a_{2k}X_2 + \cdots + a_{pk}X_p,
$$

donde el vector

$$
\mathbf{a}_k = (a_{1k}, a_{2k}, \dots, a_{pk})'
$$

es el **autovector** correspondiente al $k$-ésimo **autovalor**
$\lambda_k$ de la matriz de covarianzas (o correlaciones) del conjunto
de variables originales.

Los autovalores cumplen:

$$
\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_p \ge 0,
$$

y cada $\lambda_k$ representa la **varianza explicada** por el
componente principal $Y_k$.\
El primer componente principal captura la mayor varianza posible; el
segundo captura la mayor varianza restante bajo la condición de ser
**ortogonal** al primero, y así sucesivamente.

## Descripción general del método

Existen dos enfoques principales para comprender el método del Análisis
de Componentes Principales (ACP). El primero, tradicionalmente utilizado
en estadística, consiste en construir los componentes en las direcciones
donde la matriz de datos $X$ presenta la **máxima varianza**. Bajo este
enfoque, el primer componente principal es la combinación lineal de las
variables que captura la mayor variabilidad posible; el segundo
componente captura la mayor variabilidad restante bajo la condición de
ser ortogonal al primero, y así sucesivamente. Para ello se emplean los
autovalores y autovectores de la matriz de covarianzas o correlaciones,
lo que garantiza que los componentes sean no correlacionados y estén
ordenados según la varianza explicada.

El segundo enfoque proviene del aprendizaje estadístico moderno, donde
el ACP se interpreta como un problema de optimización que busca la mejor
aproximación de la matriz de datos con menor dimensión. Esta
aproximación se obtiene mediante la descomposición en valores singulares
(SVD), que produce las mismas direcciones de variabilidad máxima que el
enfoque clásico.

Ambos enfoques producen la misma solución: los componentes principales
corresponden a las direcciones de máxima varianza de $X$ y
simultáneamente a las direcciones que generan la mejor aproximación de
rango reducido de la matriz de datos. Esta equivalencia explica la
solidez del ACP y su importancia tanto en la estadística clásica como en
el aprendizaje automático.

## Supuestos del método

El Análisis de Componentes Principales (ACP) se basa en varios supuestos
que garantizan la validez de su interpretación y de sus resultados:

1.  **Relación lineal entre variables:**\
    El ACP asume que las relaciones entre las variables pueden
    describirse adecuadamente mediante combinaciones lineales. No es
    adecuado para capturar relaciones no lineales.

2.  **Escalas comparables entre variables:**\
    Dado que la varianza es sensible a la escala de medición, las
    variables deben estar estandarizadas cuando poseen unidades o
    magnitudes muy diferentes.

3.  **Varianza significativa en las variables:**\
    Las variables deben presentar variabilidad suficiente; variables
    casi constantes no aportan información y distorsionan los
    componentes.

4.  **Número adecuado de observaciones:**\
    Se recomienda contar con un número de observaciones
    considerablemente mayor que el número de variables para obtener
    estimaciones estables de la matriz de covarianzas o correlaciones.

5.  **Ausencia de multicolinealidad perfecta:**\
    Aunque el ACP maneja bien la colinealidad, no puede aplicarse cuando
    algunas variables son combinaciones lineales exactas de otras, pues
    la matriz de covarianzas sería singular.
    
6. **Normalidad multivariada (deseable pero no estrictamente necesaria):**  
   El ACP no requiere que las variables sigan una distribución normal multivariada    para ser calculado; sin embargo, este supuesto es útil cuando se desea realizar    inferencias estadísticas o interpretar los componentes dentro de un modelo        probabilístico.

En conjunto, estos supuestos aseguran que el ACP proporcione componentes interpretables y representativos de la estructura interna de los datos. El cumplimiento de estos principios contribuye a mejorar la estabilidad y la calidad de los resultados obtenidos.

## Las componentes principales como direcciones de máxima varianza

## Contexto inicial y preparación de datos

Partimos de una matriz de datos original:

```{r}
datos_filtrados <- datos_filtrados
# ANTES: Verificar cantidad de NA por variable
cat("=== CANTIDAD DE NA POR VARIABLE (ANTES) ===\n")
na_antes <- sapply(datos_filtrados, function(x) sum(is.na(x)))
print(na_antes)

```

$$
X_{n \times p} = \{x_{ij}\}
$$ donde: - $n$: número de objetos (individuos, ciudades, etc.) - $p$:
número de variables - $x_{ij}$: valor de la variable $j$ en el objeto
$i$

### Estadísticos básicos por variable:

-   Media: $\bar{x}_j = \frac{1}{n}\sum_{i=1}^n x_{ij}$
-   Varianza: $s_j^2 = \frac{1}{n}\sum_{i=1}^n (x_{ij} - \bar{x}_j)^2$

### **Matriz de datos centrados y estandarizados:**

$$
Y_{n \times p} = \{y_{ij}\} = \left\{\frac{x_{ij} - \bar{x}_j}{s_j}\right\}
$$

**Propiedades de Y:** - Media cero:
$\bar{Y}_j = \frac{1}{n}\sum_{i=1}^n y_{ij} = 0$ - Varianza unitaria:
$var(Y_j) = \frac{1}{n}\sum_{i=1}^n y_{ij}^2 = 1$

------------------------------------------------------------------------

## Representación de la matriz Y

Podemos ver $Y$ de dos formas:

1.  **Como vectores fila:** $y_i' = (y_{i1}, y_{i2}, \ldots, y_{ip})$
    para $i = 1,\ldots,n$\
    → Análisis de similitudes entre objetos

2.  **Como vectores columna:** $Y_j = (y_{1j}, y_{2j}, \ldots, y_{nj})'$
    para $j = 1,\ldots,p$\
    → Análisis de asociaciones entre variables

------------------------------------------------------------------------

## Primera componente principal: dirección de máxima varianza

### **Objetivo:**

Encontrar un vector de ponderaciones $u_1' = (u_{11}, \ldots, u_{p1})$
normalizado ($u_1'u_1 = 1$) que maximice la varianza de la combinación
lineal: $$
z_{i1} = u_1'y_i = \sum_{j=1}^p u_{j1}y_{ij}
$$

###Formulación del problema de optimización:

La varianza de $z_1$ es: $$
var(z_1) = \frac{1}{n}\sum_{i=1}^n z_{i1}^2 = \frac{1}{n}\sum_{i=1}^n \left(\sum_{j=1}^p u_{j1}y_{ij}\right)^2
$$

**Problema de optimización:** $$
\max_{u_{11},\ldots,u_{p1}} \left\{ \frac{1}{n}\sum_{i=1}^n \left(\sum_{j=1}^p u_{j1}y_{ij}\right)^2 \right\}
$$ sujeto a: $\sum_{j=1}^p u_{j1}^2 = 1$

------------------------------------------------------------------------

## Resolución mediante multiplicadores de Lagrange

### Función de Lagrange:

$$
\mathcal{L}(u_1, \lambda) = u_1'Ru_1 - \lambda(u_1'u_1 - 1)
$$ donde $R = \frac{1}{n}Y'Y$ es la **matriz de correlación** (cuando
trabajamos con datos estandarizados).

### Derivadas e igualación a cero:

1.  **Derivada respecto a** $u_1$: $$
    \frac{\partial \mathcal{L}}{\partial u_1} = 2Ru_1 - 2\lambda u_1 = 0
    $$ $$
    \Rightarrow Ru_1 = \lambda u_1
    $$

2.  **Derivada respecto a** $\lambda$: $$
    \frac{\partial \mathcal{L}}{\partial \lambda} = u_1'u_1 - 1 = 0
    $$ $$
    \Rightarrow u_1'u_1 = 1
    $$

### Interpretación del resultado:

La ecuación $Ru_1 = \lambda u_1$ nos dice que: - $u_1$ es un **vector
propio** de la matriz $R$ - $\lambda$ es el **valor propio**
correspondiente

Para **maximizar la varianza**, elegimos el **mayor valor propio**
$\lambda_1$ de $R$, y su vector propio correspondiente $u_1$.

------------------------------------------------------------------------

## Segunda componente principal

### **Objetivo:**

Encontrar un segundo vector $u_2' = (u_{12}, \ldots, u_{p2})$ que: - Sea
ortogonal a $u_1$: $u_1'u_2 = 0$ - Esté normalizado: $u_2'u_2 = 1$ -
Maximice la varianza de $z_2 = Yu_2$

### **Problema de optimización:**

$$
\max_{u_2} \left\{ \frac{1}{n}\sum_{i=1}^n \left(\sum_{j=1}^p u_{j2}y_{ij}\right)^2 \right\}
$$ sujeto a: - $u_2'u_2 = 1$ - $u_1'u_2 = 0$

### **Función de Lagrange ampliada:**

$$
\mathcal{L}(u_2, \lambda_2, \gamma) = u_2'Ru_2 - \lambda_2(u_2'u_2 - 1) - \gamma u_1'u_2
$$

### **Derivadas:**

$$
\frac{\partial \mathcal{L}}{\partial u_2} = 2Ru_2 - 2\lambda_2 u_2 - \gamma u_1 = 0
$$ $$
\frac{\partial \mathcal{L}}{\partial \lambda_2} = u_2'u_2 - 1 = 0
$$ $$
\frac{\partial \mathcal{L}}{\partial \gamma} = u_1'u_2 = 0
$$

### **Demostración de que** $\gamma = 0$:

Multiplicamos la primera ecuación por $u_1'$ por la izquierda: $$
2u_1'Ru_2 - 2\lambda_2 u_1'u_2 - \gamma u_1'u_1 = 0
$$

Sabemos que: - $u_1'u_2 = 0$ (restricción de ortogonalidad) -
$u_1'u_1 = 1$ (normalización) -
$u_1'Ru_2 = u_1'(\lambda_1 u_1)'u_2 = \lambda_1 u_1'u_2 = 0$

Por tanto: $$
0 - 0 - \gamma(1) = 0 \Rightarrow \gamma = 0
$$

### **Solución:**

Con $\gamma = 0$, la ecuación se reduce a: $$
Ru_2 = \lambda_2 u_2
$$

Nuevamente, $u_2$ es un **vector propio** de $R$, y para maximizar la
varianza elegimos el **segundo mayor valor propio** $\lambda_2$.

------------------------------------------------------------------------

## Componentes principales generales

### **Para el α-ésimo componente:**

$$
z_{i\alpha} = \sum_{j=1}^p u_{j\alpha}y_{ij} = y_i'u_\alpha
$$ $$
z_\alpha = Yu_\alpha
$$ donde $u_\alpha$ es el α-ésimo vector propio de $R$ asociado al
α-ésimo mayor valor propio $\lambda_\alpha$.

### **Propiedades:**

1.  **Media cero:** $m(z_\alpha) = 0$
2.  **Varianza:** $var(z_\alpha) = \lambda_\alpha$
3.  **Ortogonalidad:** $z_\alpha' z_\beta = 0$ para $\alpha \neq \beta$

------------------------------------------------------------------------

## Interpretación geométrica

Cada componente principal $z_\alpha$ representa la **proyección** de los
datos originales sobre la dirección definida por el vector propio
$u_\alpha$.

La **varianza explicada** por cada componente es exactamente igual al
valor propio correspondiente: $$
var(z_\alpha) = \lambda_\alpha
$$

El **porcentaje de varianza explicada** por la α-ésima componente es: $$
\tau_\alpha = \frac{\lambda_\alpha}{\sum_{\alpha=1}^p \lambda_\alpha}
$$

------------------------------------------------------------------------

## Resumen del procedimiento

1.  **Centrar y estandarizar** los datos:
    $Y = \left\{\frac{x_{ij}-\bar{x}_j}{s_j}\right\}$
2.  **Calcular matriz de correlación:** $R = \frac{1}{n}Y'Y$
3.  **Descomposición espectral:** Encontrar valores y vectores propios
    de $R$
4.  **Ordenar** valores propios en forma descendente:
    $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_p$
5.  **Componentes principales:** $z_\alpha = Yu_\alpha$, donde
    $u_\alpha$ es el α-ésimo vector propio
6.  **Seleccionar** las primeras $q$ componentes que acumulen suficiente
    varianza

------------------------------------------------------------------------

Esta aproximación del ACP busca **direcciones de máxima varianza** en
los datos, proporcionando una base ortogonal óptima para representar la
información contenida en la matriz original con dimensionalidad
reducida.
