---
title: "An√°lisis de Componentes Principales"
output: html_document
date: "2025-11-25"
editor_options: 
  markdown: 
    wrap: 72
---

# 4.1.1: Las componentes principales como direcciones en las que la matriz de datos tiene la m√°xima varianza

------------------------------------------------------------------------

## Contexto inicial y preparaci√≥n de datos

Partimos de una matriz de datos original:

```{r}
datos_filtrados <- datos_filtrados
# ANTES: Verificar cantidad de NA por variable
cat("=== CANTIDAD DE NA POR VARIABLE (ANTES) ===\n")
na_antes <- sapply(datos_filtrados, function(x) sum(is.na(x)))
print(na_antes)

```

$$
X_{n \times p} = \{x_{ij}\}
$$ donde: - $n$: n√∫mero de objetos (individuos, ciudades, etc.) - $p$:
n√∫mero de variables - $x_{ij}$: valor de la variable $j$ en el objeto
$i$

### Estad√≠sticos b√°sicos por variable:

-   Media: $\bar{x}_j = \frac{1}{n}\sum_{i=1}^n x_{ij}$
-   Varianza: $s_j^2 = \frac{1}{n}\sum_{i=1}^n (x_{ij} - \bar{x}_j)^2$

### **Matriz de datos centrados y estandarizados:**

$$
Y_{n \times p} = \{y_{ij}\} = \left\{\frac{x_{ij} - \bar{x}_j}{s_j}\right\}
$$

**Propiedades de Y:** - Media cero:
$\bar{Y}_j = \frac{1}{n}\sum_{i=1}^n y_{ij} = 0$ - Varianza unitaria:
$var(Y_j) = \frac{1}{n}\sum_{i=1}^n y_{ij}^2 = 1$

------------------------------------------------------------------------

## Representaci√≥n de la matriz Y

Podemos ver $Y$ de dos formas:

1.  **Como vectores fila:** $y_i' = (y_{i1}, y_{i2}, \ldots, y_{ip})$
    para $i = 1,\ldots,n$\
    ‚Üí An√°lisis de similitudes entre objetos

2.  **Como vectores columna:** $Y_j = (y_{1j}, y_{2j}, \ldots, y_{nj})'$
    para $j = 1,\ldots,p$\
    ‚Üí An√°lisis de asociaciones entre variables

------------------------------------------------------------------------

## Primera componente principal: direcci√≥n de m√°xima varianza

### **Objetivo:**

Encontrar un vector de ponderaciones $u_1' = (u_{11}, \ldots, u_{p1})$
normalizado ($u_1'u_1 = 1$) que maximice la varianza de la combinaci√≥n
lineal: $$
z_{i1} = u_1'y_i = \sum_{j=1}^p u_{j1}y_{ij}
$$

###Formulaci√≥n del problema de optimizaci√≥n:

La varianza de $z_1$ es: $$
var(z_1) = \frac{1}{n}\sum_{i=1}^n z_{i1}^2 = \frac{1}{n}\sum_{i=1}^n \left(\sum_{j=1}^p u_{j1}y_{ij}\right)^2
$$

**Problema de optimizaci√≥n:** $$
\max_{u_{11},\ldots,u_{p1}} \left\{ \frac{1}{n}\sum_{i=1}^n \left(\sum_{j=1}^p u_{j1}y_{ij}\right)^2 \right\}
$$ sujeto a: $\sum_{j=1}^p u_{j1}^2 = 1$

------------------------------------------------------------------------

## Resoluci√≥n mediante multiplicadores de Lagrange

### Funci√≥n de Lagrange:

$$
\mathcal{L}(u_1, \lambda) = u_1'Ru_1 - \lambda(u_1'u_1 - 1)
$$ donde $R = \frac{1}{n}Y'Y$ es la **matriz de correlaci√≥n** (cuando
trabajamos con datos estandarizados).

### Derivadas e igualaci√≥n a cero:

1.  **Derivada respecto a** $u_1$: $$
    \frac{\partial \mathcal{L}}{\partial u_1} = 2Ru_1 - 2\lambda u_1 = 0
    $$ $$
    \Rightarrow Ru_1 = \lambda u_1
    $$

2.  **Derivada respecto a** $\lambda$: $$
    \frac{\partial \mathcal{L}}{\partial \lambda} = u_1'u_1 - 1 = 0
    $$ $$
    \Rightarrow u_1'u_1 = 1
    $$

### Interpretaci√≥n del resultado:

La ecuaci√≥n $Ru_1 = \lambda u_1$ nos dice que: - $u_1$ es un **vector
propio** de la matriz $R$ - $\lambda$ es el **valor propio**
correspondiente

Para **maximizar la varianza**, elegimos el **mayor valor propio**
$\lambda_1$ de $R$, y su vector propio correspondiente $u_1$.

------------------------------------------------------------------------

## Segunda componente principal

### **Objetivo:**

Encontrar un segundo vector $u_2' = (u_{12}, \ldots, u_{p2})$ que: - Sea
ortogonal a $u_1$: $u_1'u_2 = 0$ - Est√© normalizado: $u_2'u_2 = 1$ -
Maximice la varianza de $z_2 = Yu_2$

### **Problema de optimizaci√≥n:**

$$
\max_{u_2} \left\{ \frac{1}{n}\sum_{i=1}^n \left(\sum_{j=1}^p u_{j2}y_{ij}\right)^2 \right\}
$$ sujeto a: - $u_2'u_2 = 1$ - $u_1'u_2 = 0$

### **Funci√≥n de Lagrange ampliada:**

$$
\mathcal{L}(u_2, \lambda_2, \gamma) = u_2'Ru_2 - \lambda_2(u_2'u_2 - 1) - \gamma u_1'u_2
$$

### **Derivadas:**

$$
\frac{\partial \mathcal{L}}{\partial u_2} = 2Ru_2 - 2\lambda_2 u_2 - \gamma u_1 = 0
$$ $$
\frac{\partial \mathcal{L}}{\partial \lambda_2} = u_2'u_2 - 1 = 0
$$ $$
\frac{\partial \mathcal{L}}{\partial \gamma} = u_1'u_2 = 0
$$

### **Demostraci√≥n de que** $\gamma = 0$:

Multiplicamos la primera ecuaci√≥n por $u_1'$ por la izquierda: $$
2u_1'Ru_2 - 2\lambda_2 u_1'u_2 - \gamma u_1'u_1 = 0
$$

Sabemos que: - $u_1'u_2 = 0$ (restricci√≥n de ortogonalidad) -
$u_1'u_1 = 1$ (normalizaci√≥n) -
$u_1'Ru_2 = u_1'(\lambda_1 u_1)'u_2 = \lambda_1 u_1'u_2 = 0$

Por tanto: $$
0 - 0 - \gamma(1) = 0 \Rightarrow \gamma = 0
$$

### **Soluci√≥n:**

Con $\gamma = 0$, la ecuaci√≥n se reduce a: $$
Ru_2 = \lambda_2 u_2
$$

Nuevamente, $u_2$ es un **vector propio** de $R$, y para maximizar la
varianza elegimos el **segundo mayor valor propio** $\lambda_2$.

------------------------------------------------------------------------

## Componentes principales generales

### **Para el Œ±-√©simo componente:**

$$
z_{i\alpha} = \sum_{j=1}^p u_{j\alpha}y_{ij} = y_i'u_\alpha
$$ $$
z_\alpha = Yu_\alpha
$$ donde $u_\alpha$ es el Œ±-√©simo vector propio de $R$ asociado al
Œ±-√©simo mayor valor propio $\lambda_\alpha$.

### **Propiedades:**

1.  **Media cero:** $m(z_\alpha) = 0$
2.  **Varianza:** $var(z_\alpha) = \lambda_\alpha$
3.  **Ortogonalidad:** $z_\alpha' z_\beta = 0$ para $\alpha \neq \beta$

------------------------------------------------------------------------

## Interpretaci√≥n geom√©trica

Cada componente principal $z_\alpha$ representa la **proyecci√≥n** de los
datos originales sobre la direcci√≥n definida por el vector propio
$u_\alpha$.

La **varianza explicada** por cada componente es exactamente igual al
valor propio correspondiente: $$
var(z_\alpha) = \lambda_\alpha
$$

El **porcentaje de varianza explicada** por la Œ±-√©sima componente es: $$
\tau_\alpha = \frac{\lambda_\alpha}{\sum_{\alpha=1}^p \lambda_\alpha}
$$

------------------------------------------------------------------------

## Resumen del procedimiento

1.  **Centrar y estandarizar** los datos:
    $Y = \left\{\frac{x_{ij}-\bar{x}_j}{s_j}\right\}$
2.  **Calcular matriz de correlaci√≥n:** $R = \frac{1}{n}Y'Y$
3.  **Descomposici√≥n espectral:** Encontrar valores y vectores propios
    de $R$
4.  **Ordenar** valores propios en forma descendente:
    $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_p$
5.  **Componentes principales:** $z_\alpha = Yu_\alpha$, donde
    $u_\alpha$ es el Œ±-√©simo vector propio
6.  **Seleccionar** las primeras $q$ componentes que acumulen suficiente
    varianza

------------------------------------------------------------------------

Esta aproximaci√≥n del ACP busca **direcciones de m√°xima varianza** en
los datos, proporcionando una base ortogonal √≥ptima para representar la
informaci√≥n contenida en la matriz original con dimensionalidad
reducida.

------------------------------------------------------------------------

#Las componentes principales como aproximaci√≥n por m√≠nimos cuadrados de la matriz de datos

------------------------------------------------------------------------
A diferencia del enfoque de la secci√≥n anterior donde las componentes principales se introdujeron como las direcciones en las que los datos presentan la m√°xima variabilidad aqu√≠ se presenta una perspectiva equivalente pero basada en aproximar la matriz de datos mediante un modelo de dimensiones reducidas, utilizando el criterio de m√≠nimos cuadrados.

El punto de partida es nuevamente la matriz estandarizada:


$$
Y_{n \times p} = \{y_{ij}\} = \left\{\frac{x_{ij} - \bar{x}_j}{s_j}\right\}
$$

El objetivo es representar a $ùëå$ mediante una versi√≥n ‚Äúsimplificada‚Äù, es decir, utilizando solo unas pocas componentes principales. Esta idea se formaliza construyendo una aproximaci√≥n de rango reducido de la forma:

$$
\tilde{\mathbf{Y}}^{(q)} = \mathbf{Z}_q \mathbf{U}'_q
$$

donde:

- $\mathbf{U}_q$ es la matriz formada por los primeros $q$ vectores propios de $\mathbf{R}$, de dimensi√≥n $p \times q$.
 
- $\mathbf{Z}_q = \mathbf{Y}\mathbf{U}_q$ contiene los scores o componentes principales, de tama√±o $n \times q$.

- $q < p$ es el n√∫mero de dimensiones deseadas.


Esta representaci√≥n indica que cualquier fila de $\mathbf{Y}$, originalmente un vector en $\mathbb{R}^p$, se aproxima mediante una combinaci√≥n lineal de solo $q$ direcciones ortogonales. 
------------------------------------------------------------------------
##Componente principal
Entonces igual que antes se sabe que una componente es una combinaci√≥n lineal: 
$$
Z_1 = u_{11}X_1 + u_{21}X_2 + \dots + u_{p1}X_p
$$

con 
$$
\sum_{j=1}^p u_{j1}^2 = 1
$$

------------------------------------------------------------------------
##Primera componente 

$$
z_{i1} = \mathbf{u}'_1 \mathbf{y}_i = \sum_{j=1}^p u_{j1} y_{ij}
$$

- $y_{ij}$: datos centrados y estandarizados.
- $\mathbf{u}_1$: vector de pesos (vector propio).
- $z_{i1}$: proyecci√≥n de cada individuo sobre la direcci√≥n de la primera componente.


Como cada columna de $\mathbf{Y}$ tiene media $0$:
- la componente tambi√©n tiene media cero.
- Su varianza es:
$$
\text{var}(\mathbf{z}_1) = \frac{1}{n} \sum_{i=1}^n z_{i1}^2
$$

------------------------------------------------------------------------
##Problema de optimizaci√≥n 

$$
\max_{\mathbf{u}_1} \frac{1}{n} \sum_{i=1}^n \left( \sum_{j=1}^p u_{j1} y_{ij} \right)^2 \quad \text{sujeto a} \quad \sum_{j=1}^p u_{j1}^2 = 1
$$
------------------------------------------------------------------------
##Interpretaci√≥n geom√©trica 
En este caso se tiene que La primera componente es la recta que minimiza las distancias cuadr√°ticas a los puntos. En donde esa recta pasa por el origen y tiene direcci√≥n $u_1$.

Tambien se puede decir que la raz√≥n por la que esta recta es √≥ptima es que minimiza la suma de las distancias cuadr√°ticas entre cada punto original y su proyecci√≥n sobre la recta. Lo cual es equivalente a la definici√≥n basada en maximizar la varianza explicada.

![Interpretaci√≥n geometrica de una componente principal](D:/imagen_1.png)

Si $z_{i1}$ es la coordenada del dato $\mathbf{y}_i$ sobre la primera componente, entonces su proyecci√≥n sobre la recta es:
$$
\hat{\mathbf{y}}_i = z_{i1} \mathbf{u}_1
$$
Esta proyecci√≥n es la mejor aproximaci√≥n posible del punto original usando solo una dimensi√≥n.
------------------------------------------------------------------------
##Aproximaci√≥n usando q componentes
De esta manera el ACP permite representar cada observaci√≥n original mediante una combinaci√≥n de solo las primeras $q$ componentes principales, reduciendo la dimensi√≥n del problema sin perder demasiada informaci√≥n.

Para cada dato original $y_{ij}$, su aproximaci√≥n utilizando √∫nicamente $q$ componentes est√° dada por:
$$
\hat{y}_{ij} = \sum_{r=1}^{q} z_{ir} u_{jr},
$$
donde:

-  $z_{ir}$ son los \textbf{scores} de la observaci√≥n $i$ sobre la componente $r$, es decir, indican qu√© tan lejos est√° el individuo en esa direcci√≥n.
-  $u_{jr}$ son los \textbf{loadings}, que describen c√≥mo contribuye la variable $j$ en la componente $r$.

Asi, esta descomposici√≥n muestra que el ACP reconstruye los datos como la multiplicaci√≥n:
$$
\text{Datos} \approx \text{Scores} \times \text{Loadings}^{\text{T}}.
$$
esta aproximaci√≥n corresponde a proyectar los datos sobre el subespacio generado por los primeros $q$ vectores propios, lo cual constituye la mejor aproximaci√≥n posible en el sentido de m√≠nimos cuadrados. Esto debido a que el ACP coincide con la descomposici√≥n espectral, asi, los vectores propios asociados a los valores propios m√°s grandes generan la mejor aproximaci√≥n posible de rango $q$.
------------------------------------------------------------------------
##Calidad de la aproximaci√≥n 
Para ver que tan buena seria esta aproximaci√≥n se puede pensar en analizar si las componentes capturan mucha o poca varianza, para esto, la varianza total (SCT) esta dada por : 
$$
\sum_{j=1}^{p} s_j^2 = \frac{1}{n} \sum_{j=1}^{p} \sum_{i=1}^{n} y_{ij}^2,
$$
y para calcular la varianza explicada por la $r$-esima componente, la varianza de esos scores es:
$$
\frac{1}{n} \sum_{i=1}^{n} z_{ir}^2 = \frac{1}{n} \sum_{i=1}^{n} \left(\sum_{j=1}^{p} u_{jr} y_{ij}\right)^2
$$
Entonces se tiene que el porcentaje de varianza explicada por la $r$-esima componente es:  
$$
\frac{\sum_{i=1}^{n} z_{ir}^2}{\sum_{j=1}^{p} \sum_{i=1}^{n} y_{ij}^2}.
$$
Una perspectiva geom√©trica clave es que la variabilidad total de cada observaci√≥n puede descomponerse exactamente en dos partes, la varianza explicada en las componentes principales mas la explicada por las diferencias entre los datos y su aproximaci√≥n que es justamente la que queda sin explicar, de esta manera:

![Descomposici√≥n de sumas de cuadrados](D:/imagen_2.png)
Esto es: 
$$
\sum_{j=1}^{p} \frac{1}{n} \sum_{i=1}^{n} y_{ij}^2 = \sum_{r=1}^{q} \frac{1}{n} \sum_{i=1}^{n} z_{ir}^2 + \sum_{j=1}^{p} \frac{1}{n} \sum_{i=1}^{n} \left(y_{ij} - \sum_{r=1}^{q} z_{ir} u_{jr}\right)^2
$$
ahora dividiendo ambas partes de la ecuacion por $\sum_{j=1}^{p} \frac{1}{n} \sum_{i=1}^{n} y_{ij}^2$ y despejando se puede observar que: 
$$
\frac{\sum_{r=1}^{q} \sum_{i=1}^{n} z_{ir}^2}{\sum_{j=1}^{p} \sum_{i=1}^{n} y_{ij}^2} = 1 - \frac{\sum_{j=1}^{p} \sum_{i=1}^{n} \left(y_{ij} - \sum_{r=1}^{q} z_{ir} u_{jr}\right)^2}{\sum_{j=1}^{p} \sum_{i=1}^{n} y_{ij}^2}
$$
que esto es justamente la fracci√≥n de la variabilidad total que es explicada por los primeros $q$ componentes principales, esto es lo que se conoce como
$$\tau_q = 1 - \frac{SCR}{SCT} $$
donde $SCR$ es la suma de cuadrados residual y $SCT$ es la suma de cuadrados total y se puede decir que si el $SCR$ es peque√±o, significa que la aproximaci√≥n con $q$ componentes es muy buena, en consecuencia, $\tau_q$ ser√° grande, indicando que los componentes capturan una alta proporci√≥n de la informaci√≥n original.

------------------------------------------------------------------------

#Equivalencia entre el enfoque de direcciones de m√°xima varianza y el de aproximaci√≥n por m√≠nimos cuadrados
------------------------------------------------------------------------
Partiendo de la expansi√≥n en valores singulares de la matriz estandarizada
$$
\mathbf{Y} = \sum_{\alpha=1}^{p} \sqrt{\lambda_{\alpha}} \mathbf{v}_{\alpha} \mathbf{u}_{\alpha}',
$$
cada entrada de $\mathbf{Y}$ se escribe como
$$
y_{ij} = \sum_{\alpha=1}^{p} \sqrt{\lambda_{\alpha}} v_{i\alpha} u_{j\alpha}.
$$
luego se sabe que la aproximaci√≥n de rango $q$ se obtiene truncando esta suma en los primeros $q$ t√©rminos:
$$
\hat{y}_{ij} = \sum_{\alpha=1}^{q} \sqrt{\lambda_{\alpha}} v_{i\alpha} u_{j\alpha}.
$$
Entonces, se puede ver que el residuo de la aproximaci√≥n es exactamente:
$$
y_{ij} - \hat{y}_{ij} = \sum_{\alpha=q+1}^{p} \sqrt{\lambda_{\alpha}} v_{i\alpha} u_{j\alpha}.
$$
Al tomar sumas de cuadrados sobre todas las observaciones y variables y usar la ortonormalidad de los autovectores, se obtiene que la suma de cuadrados residual queda dada por la suma de los autovalores descartados, esto es; 
$$
SCR = \sum_{j=1}^{p} \sum_{i=1}^{n} (y_{ij} - \hat{y}_{ij})^2 = \sum_{\alpha=q+1}^{p} \lambda_{\alpha},
$$
mientras que la suma de cuadrados total satisface $SCT = \sum_{\alpha=1}^{p} \lambda_{\alpha}$. De aqu√≠ se puede deduce que minimizar la suma de cuadrados residual equivale a maximizar la varianza explicada por las $q$ primeras componentes;
$$
\tau_q = \frac{\sum_{\alpha=1}^{q} \lambda_{\alpha}}{\sum_{\alpha=1}^{p} \lambda_{\alpha}} = 1 - \frac{SCR}{SCT}.
$$
En consecuencia, las direcciones que resuelven el problema de m√°xima varianza y las que resuelven el problema de aproximaci√≥n por m√≠nimos cuadrados coinciden, asi, ambas son los vectores propios de $\mathbf{Y}'\mathbf{Y}$ asociados a los mayores autovalores, entonces las soluciones son equivalentes.

------------------------------------------------------------------------