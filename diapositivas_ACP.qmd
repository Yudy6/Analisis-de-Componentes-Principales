---
title: "Análisis de Componentes Principales"
subtitle: "Explorando la reducción de dimensionalidad"
author: "Tu Nombre"
date: "26 de noviembre de 2025"
format:
  revealjs:
    theme: default
    transition: "slide"
    slide-number: true
    center: false
    progress: true
    hash: true
    width: 1200
    height: 800
    font-family: "Inter, sans-serif"
    highlight-style: "github-dark"
    self-contained: true
---


# 4.1.1: Las componentes principales como direcciones en las que la matriz de datos tiene la máxima varianza

---

## Contexto inicial y preparación de datos

Partimos de una matriz de datos original:

$$
X_{n \times p} = \{x_{ij}\}
$$
donde:
- $n$: número de objetos (individuos, ciudades, etc.)
- $p$: número de variables
- $x_{ij}$: valor de la variable $j$ en el objeto $i$

### Estadísticos básicos por variable:
- Media: $\bar{x}_j = \frac{1}{n}\sum_{i=1}^n x_{ij}$
- Varianza: $s_j^2 = \frac{1}{n}\sum_{i=1}^n (x_{ij} - \bar{x}_j)^2$

### **Matriz de datos centrados y estandarizados:**
$$
Y_{n \times p} = \{y_{ij}\} = \left\{\frac{x_{ij} - \bar{x}_j}{s_j}\right\}
$$

**Propiedades de Y:**
- Media cero: $\bar{Y}_j = \frac{1}{n}\sum_{i=1}^n y_{ij} = 0$
- Varianza unitaria: $var(Y_j) = \frac{1}{n}\sum_{i=1}^n y_{ij}^2 = 1$

---

##  Representación de la matriz Y

Podemos ver $Y$ de dos formas:

1. **Como vectores fila:** $y_i' = (y_{i1}, y_{i2}, \ldots, y_{ip})$ para $i = 1,\ldots,n$  
   → Análisis de similitudes entre objetos

2. **Como vectores columna:** $Y_j = (y_{1j}, y_{2j}, \ldots, y_{nj})'$ para $j = 1,\ldots,p$  
   → Análisis de asociaciones entre variables

---

## Primera componente principal: dirección de máxima varianza

### **Objetivo:**
Encontrar un vector de ponderaciones $u_1' = (u_{11}, \ldots, u_{p1})$ normalizado ($u_1'u_1 = 1$) que maximice la varianza de la combinación lineal:
$$
z_{i1} = u_1'y_i = \sum_{j=1}^p u_{j1}y_{ij}
$$

###Formulación del problema de optimización:

La varianza de $z_1$ es:
$$
var(z_1) = \frac{1}{n}\sum_{i=1}^n z_{i1}^2 = \frac{1}{n}\sum_{i=1}^n \left(\sum_{j=1}^p u_{j1}y_{ij}\right)^2
$$

**Problema de optimización:**
$$
\max_{u_{11},\ldots,u_{p1}} \left\{ \frac{1}{n}\sum_{i=1}^n \left(\sum_{j=1}^p u_{j1}y_{ij}\right)^2 \right\}
$$
sujeto a: $\sum_{j=1}^p u_{j1}^2 = 1$

---

## Resolución mediante multiplicadores de Lagrange

### Función de Lagrange:
$$
\mathcal{L}(u_1, \lambda) = u_1'Ru_1 - \lambda(u_1'u_1 - 1)
$$
donde $R = \frac{1}{n}Y'Y$ es la **matriz de correlación** (cuando trabajamos con datos estandarizados).

### Derivadas e igualación a cero:

1. **Derivada respecto a $u_1$:**
$$
\frac{\partial \mathcal{L}}{\partial u_1} = 2Ru_1 - 2\lambda u_1 = 0
$$
$$
\Rightarrow Ru_1 = \lambda u_1
$$

2. **Derivada respecto a $\lambda$:**
$$
\frac{\partial \mathcal{L}}{\partial \lambda} = u_1'u_1 - 1 = 0
$$
$$
\Rightarrow u_1'u_1 = 1
$$

### Interpretación del resultado:

La ecuación $Ru_1 = \lambda u_1$ nos dice que:
- $u_1$ es un **vector propio** de la matriz $R$
- $\lambda$ es el **valor propio** correspondiente

Para **maximizar la varianza**, elegimos el **mayor valor propio** $\lambda_1$ de $R$, y su vector propio correspondiente $u_1$.

---

## Segunda componente principal

### **Objetivo:**
Encontrar un segundo vector $u_2' = (u_{12}, \ldots, u_{p2})$ que:
- Sea ortogonal a $u_1$: $u_1'u_2 = 0$
- Esté normalizado: $u_2'u_2 = 1$
- Maximice la varianza de $z_2 = Yu_2$

### **Problema de optimización:**
$$
\max_{u_2} \left\{ \frac{1}{n}\sum_{i=1}^n \left(\sum_{j=1}^p u_{j2}y_{ij}\right)^2 \right\}
$$
sujeto a:
- $u_2'u_2 = 1$
- $u_1'u_2 = 0$

### **Función de Lagrange ampliada:**
$$
\mathcal{L}(u_2, \lambda_2, \gamma) = u_2'Ru_2 - \lambda_2(u_2'u_2 - 1) - \gamma u_1'u_2
$$

### **Derivadas:**
$$
\frac{\partial \mathcal{L}}{\partial u_2} = 2Ru_2 - 2\lambda_2 u_2 - \gamma u_1 = 0
$$
$$
\frac{\partial \mathcal{L}}{\partial \lambda_2} = u_2'u_2 - 1 = 0
$$
$$
\frac{\partial \mathcal{L}}{\partial \gamma} = u_1'u_2 = 0
$$

### **Demostración de que $\gamma = 0$:**

Multiplicamos la primera ecuación por $u_1'$ por la izquierda:
$$
2u_1'Ru_2 - 2\lambda_2 u_1'u_2 - \gamma u_1'u_1 = 0
$$

Sabemos que:
- $u_1'u_2 = 0$ (restricción de ortogonalidad)
- $u_1'u_1 = 1$ (normalización)
- $u_1'Ru_2 = u_1'(\lambda_1 u_1)'u_2 = \lambda_1 u_1'u_2 = 0$

Por tanto:
$$
0 - 0 - \gamma(1) = 0 \Rightarrow \gamma = 0
$$

### **Solución:**
Con $\gamma = 0$, la ecuación se reduce a:
$$
Ru_2 = \lambda_2 u_2
$$

Nuevamente, $u_2$ es un **vector propio** de $R$, y para maximizar la varianza elegimos el **segundo mayor valor propio** $\lambda_2$.

---

## Componentes principales generales

### **Para el α-ésimo componente:**
$$
z_{i\alpha} = \sum_{j=1}^p u_{j\alpha}y_{ij} = y_i'u_\alpha
$$
$$
z_\alpha = Yu_\alpha
$$
donde $u_\alpha$ es el α-ésimo vector propio de $R$ asociado al α-ésimo mayor valor propio $\lambda_\alpha$.

### **Propiedades:**
1. **Media cero:** $m(z_\alpha) = 0$
2. **Varianza:** $var(z_\alpha) = \lambda_\alpha$
3. **Ortogonalidad:** $z_\alpha' z_\beta = 0$ para $\alpha \neq \beta$

---

## Interpretación geométrica

Cada componente principal $z_\alpha$ representa la **proyección** de los datos originales sobre la dirección definida por el vector propio $u_\alpha$.

La **varianza explicada** por cada componente es exactamente igual al valor propio correspondiente:
$$
var(z_\alpha) = \lambda_\alpha
$$

El **porcentaje de varianza explicada** por la α-ésima componente es:
$$
\tau_\alpha = \frac{\lambda_\alpha}{\sum_{\alpha=1}^p \lambda_\alpha}
$$

---

## Resumen del procedimiento

1. **Centrar y estandarizar** los datos: $Y = \left\{\frac{x_{ij}-\bar{x}_j}{s_j}\right\}$
2. **Calcular matriz de correlación:** $R = \frac{1}{n}Y'Y$
3. **Descomposición espectral:** Encontrar valores y vectores propios de $R$
4. **Ordenar** valores propios en forma descendente: $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_p$
5. **Componentes principales:** $z_\alpha = Yu_\alpha$, donde $u_\alpha$ es el α-ésimo vector propio
6. **Seleccionar** las primeras $q$ componentes que acumulen suficiente varianza

---

Esta aproximación del ACP busca **direcciones de máxima varianza** en los datos, proporcionando una base ortogonal óptima para representar la información contenida en la matriz original con dimensionalidad reducida.