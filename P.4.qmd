---
title: "An√°lisis de Componentes Principales"
subtitle: "Universidad Nacional de Colombia"
author:

  - "Yudy Vanesa Puerres Rosero (ypuerresr@unal.edu.co)"
  - "Camila Andrea Ayala Camargo (caayala@unal.edu.co)"
  - "Karen Liliana Barrantes Quiroga (kbarrantes@unal.edu.co)"
  - "Laura Katherine Mart√≠nez Castiblanco (laumartinezca@unal.edu.co)"
  - "Fredy Arley Urrea Cifuentes (furreac@unal.edu.co)"

date: ""      
lang: es

format:
  pdf:
    documentclass: article
    fontsize: 11pt
    geometry: "margin=2.5cm"
    linestretch: 1.1
    indent: true

header-includes: |
  \setlength{\parskip}{0em}          % sin espacio extra entre p√°rrafos
  \setlength{\parindent}{1.5em}      % con sangr√≠a
  \date{1 de noviembre de 2025}      % fecha tal cual quieres que salga
---

# Introducci√≥n

En la investigaci√≥n aplicada, especialmente en ciencias sociales, biolog√≠a, econom√≠a e ingenier√≠a es com√∫n trabajar con conjuntos de datos de alta dimensionalidad. Aunque un mayor n√∫mero de variables puede aportar informaci√≥n valiosa, su an√°lisis simult√°neo presenta dificultades como el aumento de par√°metros a estimar, el riesgo de multicolinealidad y la complejidad para interpretar la estructura subyacente de los datos. Esto hace necesario emplear m√©todos que resuman la informaci√≥n esencial sin perder las caracter√≠sticas relevantes del conjunto original.

El An√°lisis de Componentes Principales (ACP) es una t√©cnica multivariante dise√±ada precisamente para reducir la dimensionalidad. Transforma las variables originales en un nuevo conjunto menor de variables no correlacionadas, denominadas componentes principales. Estas componentes son combinaciones lineales de las variables originales, construidas de modo que capturen la m√°xima varianza posible, concentrando as√≠ la mayor cantidad de informaci√≥n.

## Definici√≥n:

El An√°lisis de Componentes Principales (ACP) es una t√©cnica estad√≠stica multivariante cuyo objetivo es transformar un conjunto de variables originales $X_1, X_2, \dots, X_p$ en un nuevo conjunto de variables no correlacionadas llamadas **componentes principales**. Estas nuevas variables son combinaciones lineales de las variables originales y se construyen de forma que capturen la m√°xima varianza posible, es decir, la mayor cantidad de informaci√≥n contenida en los datos.

Formalmente, el $k$-√©simo componente principal se define como:

$$
Y_k = a_{1k}X_1 + a_{2k}X_2 + \cdots + a_{pk}X_p,
$$

donde el vector

$$
\mathbf{a}_k = (a_{1k}, a_{2k}, \dots, a_{pk})'
$$

es el **autovector** correspondiente al $k$-√©simo **autovalor** $\lambda_k$ de la matriz de covarianzas (o correlaciones) del conjunto de variables originales.

Los autovalores cumplen:

$$
\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_p \ge 0,
$$

y cada $\lambda_k$ representa la **varianza explicada** por el componente principal $Y_k$.\
El primer componente principal captura la mayor varianza posible; el segundo captura la mayor varianza restante bajo la condici√≥n de ser **ortogonal** al primero, y as√≠ sucesivamente.

## Fundamento te√≥rico del An√°lisis de componentes principales

Existen dos enfoques principales para comprender el m√©todo del An√°lisis de Componentes Principales (ACP). El primero, tradicionalmente utilizado en estad√≠stica, consiste en construir los componentes en las direcciones donde la matriz de datos $X$ presenta la **m√°xima varianza**. Bajo este enfoque, el primer componente principal es la combinaci√≥n lineal de las variables que captura la mayor variabilidad posible; el segundo componente captura la mayor variabilidad restante bajo la condici√≥n de ser ortogonal al primero, y as√≠ sucesivamente. Para ello se emplean los autovalores y autovectores de la matriz de covarianzas o correlaciones, lo que garantiza que los componentes sean no correlacionados y est√©n ordenados seg√∫n la varianza explicada.

El segundo enfoque proviene del aprendizaje estad√≠stico moderno, donde el ACP se interpreta como un problema de optimizaci√≥n que busca la mejor aproximaci√≥n de la matriz de datos con menor dimensi√≥n. Esta aproximaci√≥n se obtiene mediante la descomposici√≥n en valores singulares (SVD), que produce las mismas direcciones de variabilidad m√°xima que el enfoque cl√°sico.

Ambos enfoques producen la misma soluci√≥n: los componentes principales corresponden a las direcciones de m√°xima varianza de $X$ y simult√°neamente a las direcciones que generan la mejor aproximaci√≥n de rango reducido de la matriz de datos. Esta equivalencia explica la solidez del ACP y su importancia tanto en la estad√≠stica cl√°sica como en el aprendizaje autom√°tico.

## Supuestos del m√©todo

El An√°lisis de Componentes Principales (ACP) se basa en varios supuestos que garantizan la validez de su interpretaci√≥n y de sus resultados:

1.  **Relaci√≥n lineal entre variables:**

El ACP identifica direcciones de m√°xima varianza mediante combinaciones lineales. Por ello, se asume que la estructura subyacente del conjunto de datos puede capturarse adecuadamente a trav√©s de relaciones lineales entre las variables. Relaciones no lineales no son detectadas por este m√©todo.

2.  **Escalas comparables entre variables:** Debido a que el ACP maximiza la varianza, las variables deben tener escalas similares para evitar que aquellas con valores mayores dominen los componentes. Cuando las unidades son heterog√©neas, se recomienda aplicar estandarizaci√≥n

3.  **Varianza significativa en las variables:** Variables con varianza muy baja o casi constantes no contribuyen informaci√≥n relevante y pueden distorsionar la estructura de los componentes. El ACP requiere que las variables tengan variabilidad apreciable.

4.  **N√∫mero adecuado de observaciones:** Para obtener estimaciones estables de la matriz de covarianzas o correlaciones, es recomendable que el n√∫mero de observaciones supere ampliamente al n√∫mero de variables.

5.  **Ausencia de multicolinealidad perfecta:** Aunque el ACP maneja bien la colinealidad, no puede aplicarse cuando algunas variables son combinaciones lineales exactas de otras, ya que la matriz de covarianzas se vuelve singular y no puede descomponerse.

6.  **Normalidad multivariada (deseable pero no estrictamente necesaria):** El ACP no requiere que las variables sigan una distribuci√≥n normal multivariada para ser calculado; sin embargo, este supuesto es √∫til cuando se desea realizar inferencias estad√≠sticas o interpretar los componentes dentro de un modelo probabil√≠stico.

En conjunto, estos supuestos aseguran que el ACP proporcione componentes interpretables y representativos de la estructura interna de los datos. El cumplimiento de estos principios contribuye a mejorar la estabilidad y la calidad de los resultados obtenidos.

## Las componentes principales como direcciones de m√°xima varianza

## Contexto inicial y preparaci√≥n de datos

Partimos de una matriz de datos original:

$$
X_{n \times p} = \{x_{ij}\}
$$ donde: - $n$: n√∫mero de objetos (individuos, ciudades, etc.) - $p$: n√∫mero de variables - $x_{ij}$: valor de la variable $j$ en el objeto $i$

### Estad√≠sticos b√°sicos por variable:

-   Media: $\bar{x}_j = \frac{1}{n}\sum_{i=1}^n x_{ij}$
-   Varianza: $s_j^2 = \frac{1}{n}\sum_{i=1}^n (x_{ij} - \bar{x}_j)^2$

### **Matriz de datos centrados y estandarizados:**

$$
Y_{n \times p} = \{y_{ij}\} = \left\{\frac{x_{ij} - \bar{x}_j}{s_j}\right\}
$$

**Propiedades de Y:** - Media cero: $\bar{Y}_j = \frac{1}{n}\sum_{i=1}^n y_{ij} = 0$ - Varianza unitaria: $var(Y_j) = \frac{1}{n}\sum_{i=1}^n y_{ij}^2 = 1$

## Representaci√≥n de la matriz Y

Podemos ver $Y$ de dos formas:

1.  **Como vectores fila:** $y_i' = (y_{i1}, y_{i2}, \ldots, y_{ip})$ para $i = 1,\ldots,n$\
    ‚Üí An√°lisis de similitudes entre objetos

2.  **Como vectores columna:** $Y_j = (y_{1j}, y_{2j}, \ldots, y_{nj})'$ para $j = 1,\ldots,p$\
    ‚Üí An√°lisis de asociaciones entre variables

## Primera componente principal: direcci√≥n de m√°xima varianza

### **Objetivo:**

Encontrar un vector de ponderaciones $u_1' = (u_{11}, \ldots, u_{p1})$ normalizado ($u_1'u_1 = 1$) que maximice la varianza de la combinaci√≥n lineal: $$
z_{i1} = u_1'y_i = \sum_{j=1}^p u_{j1}y_{ij}
$$

### Formulaci√≥n del problema de optimizaci√≥n:

La varianza de $z_1$ es: $$
var(z_1) = \frac{1}{n}\sum_{i=1}^n z_{i1}^2 = \frac{1}{n}\sum_{i=1}^n \left(\sum_{j=1}^p u_{j1}y_{ij}\right)^2
$$

**Problema de optimizaci√≥n:** $$
\max_{u_{11},\ldots,u_{p1}} \left\{ \frac{1}{n}\sum_{i=1}^n \left(\sum_{j=1}^p u_{j1}y_{ij}\right)^2 \right\}
$$ sujeto a: $\sum_{j=1}^p u_{j1}^2 = 1$

## Resoluci√≥n mediante multiplicadores de Lagrange

### Funci√≥n de Lagrange:

$$
\mathcal{L}(u_1, \lambda) = u_1'Ru_1 - \lambda(u_1'u_1 - 1)
$$ donde $R = \frac{1}{n}Y'Y$ es la **matriz de correlaci√≥n** (cuando trabajamos con datos estandarizados).

### Derivadas e igualaci√≥n a cero:

1.  **Derivada respecto a** $u_1$: $$
    \frac{\partial \mathcal{L}}{\partial u_1} = 2Ru_1 - 2\lambda u_1 = 0
    $$ $$
    \Rightarrow Ru_1 = \lambda u_1
    $$

2.  **Derivada respecto a** $\lambda$: $$
    \frac{\partial \mathcal{L}}{\partial \lambda} = u_1'u_1 - 1 = 0
    $$ $$
    \Rightarrow u_1'u_1 = 1
    $$

### Interpretaci√≥n del resultado:

La ecuaci√≥n $Ru_1 = \lambda u_1$ nos dice que: - $u_1$ es un **vector propio** de la matriz $R$ - $\lambda$ es el **valor propio** correspondiente

Para **maximizar la varianza**, elegimos el **mayor valor propio** $\lambda_1$ de $R$, y su vector propio correspondiente $u_1$.

## Segunda componente principal

### **Objetivo:**

Encontrar un segundo vector $u_2' = (u_{12}, \ldots, u_{p2})$ que: - Sea ortogonal a $u_1$: $u_1'u_2 = 0$ - Est√© normalizado: $u_2'u_2 = 1$ - Maximice la varianza de $z_2 = Yu_2$

### **Problema de optimizaci√≥n:**

$$
\max_{u_2} \left\{ \frac{1}{n}\sum_{i=1}^n \left(\sum_{j=1}^p u_{j2}y_{ij}\right)^2 \right\}
$$ sujeto a: - $u_2'u_2 = 1$ - $u_1'u_2 = 0$

### **Funci√≥n de Lagrange ampliada:**

$$
\mathcal{L}(u_2, \lambda_2, \gamma) = u_2'Ru_2 - \lambda_2(u_2'u_2 - 1) - \gamma u_1'u_2
$$

### **Derivadas:**

$$
\frac{\partial \mathcal{L}}{\partial u_2} = 2Ru_2 - 2\lambda_2 u_2 - \gamma u_1 = 0
$$ $$
\frac{\partial \mathcal{L}}{\partial \lambda_2} = u_2'u_2 - 1 = 0
$$ $$
\frac{\partial \mathcal{L}}{\partial \gamma} = u_1'u_2 = 0
$$

### **Demostraci√≥n de que** $\gamma = 0$:

Multiplicamos la primera ecuaci√≥n por $u_1'$ por la izquierda: $$
2u_1'Ru_2 - 2\lambda_2 u_1'u_2 - \gamma u_1'u_1 = 0
$$

Sabemos que: - $u_1'u_2 = 0$ (restricci√≥n de ortogonalidad) - $u_1'u_1 = 1$ (normalizaci√≥n) - $u_1'Ru_2 = u_1'(\lambda_1 u_1)'u_2 = \lambda_1 u_1'u_2 = 0$

Por tanto: $$
0 - 0 - \gamma(1) = 0 \Rightarrow \gamma = 0
$$

### **Soluci√≥n:**

Con $\gamma = 0$, la ecuaci√≥n se reduce a: $$
Ru_2 = \lambda_2 u_2
$$

Nuevamente, $u_2$ es un **vector propio** de $R$, y para maximizar la varianza elegimos el **segundo mayor valor propio** $\lambda_2$.

## Componentes principales generales

### **Para el Œ±-√©simo componente:**

$$
z_{i\alpha} = \sum_{j=1}^p u_{j\alpha}y_{ij} = y_i'u_\alpha
$$ $$
z_\alpha = Yu_\alpha
$$ donde $u_\alpha$ es el Œ±-√©simo vector propio de $R$ asociado al Œ±-√©simo mayor valor propio $\lambda_\alpha$.

### **Propiedades:**

1.  **Media cero:** $m(z_\alpha) = 0$
2.  **Varianza:** $var(z_\alpha) = \lambda_\alpha$
3.  **Ortogonalidad:** $z_\alpha' z_\beta = 0$ para $\alpha \neq \beta$

## Interpretaci√≥n geom√©trica

Cada componente principal $z_\alpha$ representa la **proyecci√≥n** de los datos originales sobre la direcci√≥n definida por el vector propio $u_\alpha$.

La **varianza explicada** por cada componente es exactamente igual al valor propio correspondiente: $$
var(z_\alpha) = \lambda_\alpha
$$

El **porcentaje de varianza explicada** por la Œ±-√©sima componente es: $$
\tau_\alpha = \frac{\lambda_\alpha}{\sum_{\alpha=1}^p \lambda_\alpha}
$$

## Resumen del procedimiento

1.  **Centrar y estandarizar** los datos: $Y = \left\{\frac{x_{ij}-\bar{x}_j}{s_j}\right\}$
2.  **Calcular matriz de correlaci√≥n:** $R = \frac{1}{n}Y'Y$
3.  **Descomposici√≥n espectral:** Encontrar valores y vectores propios de $R$
4.  **Ordenar** valores propios en forma descendente: $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_p$
5.  **Componentes principales:** $z_\alpha = Yu_\alpha$, donde $u_\alpha$ es el Œ±-√©simo vector propio
6.  **Seleccionar** las primeras $q$ componentes que acumulen suficiente varianza

Esta aproximaci√≥n del ACP busca **direcciones de m√°xima varianza** en los datos, proporcionando una base ortogonal √≥ptima para representar la informaci√≥n contenida en la matriz original con dimensionalidad reducida.

# Las componentes principales como aproximaci√≥n por m√≠nimos cuadrados de la matriz de datos

A diferencia del enfoque de la secci√≥n anterior donde las componentes principales se introdujeron como las direcciones en las que los datos presentan la m√°xima variabilidad aqu√≠ se presenta una perspectiva equivalente pero basada en aproximar la matriz de datos mediante un modelo de dimensiones reducidas, utilizando el criterio de m√≠nimos cuadrados.

El punto de partida es nuevamente la matriz estandarizada:

$$
Y_{n \times p} = \{y_{ij}\} = \left\{\frac{x_{ij} - \bar{x}_j}{s_j}\right\}
$$

El objetivo es representar a $ùëå$ mediante una versi√≥n ‚Äúsimplificada‚Äù, es decir, utilizando solo unas pocas componentes principales. Esta idea se formaliza construyendo una aproximaci√≥n de rango reducido de la forma:

$$
\tilde{\mathbf{Y}}^{(q)} = \mathbf{Z}_q \mathbf{U}'_q
$$

donde:

-   $\mathbf{U}_q$ es la matriz formada por los primeros $q$ vectores propios de $\mathbf{R}$, de dimensi√≥n $p \times q$.

-   $\mathbf{Z}_q = \mathbf{Y}\mathbf{U}_q$ contiene los scores o componentes principales, de tama√±o $n \times q$.

-   $q < p$ es el n√∫mero de dimensiones deseadas.

Esta representaci√≥n indica que cualquier fila de $\mathbf{Y}$, originalmente un vector en $\mathbb{R}^p$, se aproxima mediante una combinaci√≥n lineal de solo $q$ direcciones ortogonales.

## Componente principal

Entonces igual que antes se sabe que una componente es una combinaci√≥n lineal: $$
Z_1 = u_{11}X_1 + u_{21}X_2 + \dots + u_{p1}X_p
$$

con $$
\sum_{j=1}^p u_{j1}^2 = 1
$$

## Primera componente

$$
z_{i1} = \mathbf{u}'_1 \mathbf{y}_i = \sum_{j=1}^p u_{j1} y_{ij}
$$

-   $y_{ij}$: datos centrados y estandarizados.
-   $\mathbf{u}_1$: vector de pesos (vector propio).
-   $z_{i1}$: proyecci√≥n de cada individuo sobre la direcci√≥n de la primera componente.

Como cada columna de $\mathbf{Y}$ tiene media $0$: - la componente tambi√©n tiene media cero. - Su varianza es: $$
\text{var}(\mathbf{z}_1) = \frac{1}{n} \sum_{i=1}^n z_{i1}^2
$$

## Problema de optimizaci√≥n

$$ \max_{\mathbf{u}_1} \frac{1}{n} \sum_{i=1}^n \left( \sum_{j=1}^p u_{j1} y_{ij} \right)^2 \quad \text{sujeto a} \quad \sum_{j=1}^p u_{j1}^2 = 1      
   $$

## Interpretaci√≥n geom√©trica

En este caso se tiene que La primera componente es la recta que minimiza las distancias cuadr√°ticas a los puntos. En donde esa recta pasa por el origen y tiene direcci√≥n $u_1$.

Tambien se puede decir que la raz√≥n por la que esta recta es √≥ptima es que minimiza la suma de las distancias cuadr√°ticas entre cada punto original y su proyecci√≥n sobre la recta. Lo cual es equivalente a la definici√≥n basada en maximizar la varianza explicada.

![Interpretaci√≥n geometrica de una componente principal](imagen_1.png)

Si $z_{i1}$ es la coordenada del dato $\mathbf{y}_i$ sobre la primera componente, entonces su proyecci√≥n sobre la recta es: $$
\hat{\mathbf{y}}_i = z_{i1} \mathbf{u}_1
$$ Esta proyecci√≥n es la mejor aproximaci√≥n posible del punto original usando solo una dimensi√≥n.

## Aproximaci√≥n usando q componentes

De esta manera el ACP permite representar cada observaci√≥n original mediante una combinaci√≥n de solo las primeras $q$ componentes principales, reduciendo la dimensi√≥n del problema sin perder demasiada informaci√≥n.

Para cada dato original $y_{ij}$, su aproximaci√≥n utilizando √∫nicamente $q$ componentes est√° dada por: $$
\hat{y}_{ij} = \sum_{r=1}^{q} z_{ir} u_{jr},
$$ donde:

-   $z_{ir}$ son los \textbf{scores} de la observaci√≥n $i$ sobre la componente $r$, es decir, indican qu√© tan lejos est√° el individuo en esa direcci√≥n.
-   $u_{jr}$ son los \textbf{loadings}, que describen c√≥mo contribuye la variable $j$ en la componente $r$.

Asi, esta descomposici√≥n muestra que el ACP reconstruye los datos como la multiplicaci√≥n: $$
\text{Datos} \approx \text{Scores} \times \text{Loadings}^{\text{T}}.
$$ esta aproximaci√≥n corresponde a proyectar los datos sobre el subespacio generado por los primeros $q$ vectores propios, lo cual constituye la mejor aproximaci√≥n posible en el sentido de m√≠nimos cuadrados. Esto debido a que el ACP coincide con la descomposici√≥n espectral, asi, los vectores propios asociados a los valores propios m√°s grandes generan la mejor aproximaci√≥n posible de rango $q$.

## Calidad de la aproximaci√≥n

Para ver que tan buena seria esta aproximaci√≥n se puede pensar en analizar si las componentes capturan mucha o poca varianza, para esto, la varianza total (SCT) esta dada por: $$
\sum_{j=1}^{p} s_j^2 = \frac{1}{n} \sum_{j=1}^{p} \sum_{i=1}^{n} y_{ij}^2,
$$ y para calcular la varianza explicada por la $r$-esima componente, la varianza de esos scores es: $$
\frac{1}{n} \sum_{i=1}^{n} z_{ir}^2 = \frac{1}{n} \sum_{i=1}^{n} \left(\sum_{j=1}^{p} u_{jr} y_{ij}\right)^2
$$ Entonces se tiene que el porcentaje de varianza explicada por la $r$-esima componente es:

$$
\frac{\sum_{i=1}^{n} z_{ir}^2}{\sum_{j=1}^{p} \sum_{i=1}^{n} y_{ij}^2}.
$$ Una perspectiva geom√©trica clave es que la variabilidad total de cada observaci√≥n puede descomponerse exactamente en dos partes, la varianza explicada en las componentes principales mas la explicada por las diferencias entre los datos y su aproximaci√≥n que es justamente la que queda sin explicar, de esta manera:

![Descomposici√≥n de sumas de cuadrados](imagen_2.png) Esto es: $$
\sum_{j=1}^{p} \frac{1}{n} \sum_{i=1}^{n} y_{ij}^2 = \sum_{r=1}^{q} \frac{1}{n} \sum_{i=1}^{n} z_{ir}^2 + \sum_{j=1}^{p} \frac{1}{n} \sum_{i=1}^{n} \left(y_{ij} - \sum_{r=1}^{q} z_{ir} u_{jr}\right)^2
$$ ahora dividiendo ambas partes de la ecuacion por $\sum_{j=1}^{p} \frac{1}{n} \sum_{i=1}^{n} y_{ij}^2$ y despejando se puede observar que: $$
\frac{\sum_{r=1}^{q} \sum_{i=1}^{n} z_{ir}^2}{\sum_{j=1}^{p} \sum_{i=1}^{n} y_{ij}^2} = 1 - \frac{\sum_{j=1}^{p} \sum_{i=1}^{n} \left(y_{ij} - \sum_{r=1}^{q} z_{ir} u_{jr}\right)^2}{\sum_{j=1}^{p} \sum_{i=1}^{n} y_{ij}^2}
$$ que esto es justamente la fracci√≥n de la variabilidad total que es explicada por los primeros $q$ componentes principales, esto es lo que se conoce como $$\tau_q = 1 - \frac{SCR}{SCT} $$ donde $SCR$ es la suma de cuadrados residual y $SCT$ es la suma de cuadrados total y se puede decir que si el $SCR$ es peque√±o, significa que la aproximaci√≥n con $q$ componentes es muy buena, en consecuencia, $\tau_q$ ser√° grande, indicando que los componentes capturan una alta proporci√≥n de la informaci√≥n original.

# Reconstrucci√≥n Exacta de una Matriz de Datos usando Valores y Vectores Propios

En el an√°lisis multivariado, y en particular en m√©todos como el An√°lisis de Componentes Principales (ACP), es posible reconstruir exactamente una matriz de datos utilizando sus valores y vectores propios gracias a la Descomposici√≥n en Valores Singulares (DVS).

Este procedimiento descompone una matriz en el producto de tres matrices con caracter√≠sticas espec√≠ficas, lo que no solo facilita su interpretaci√≥n geom√©trica, sino que tambi√©n resulta muy √∫til para realizar reducci√≥n de dimensionalidad.

## La Descomposici√≥n en Valores Singulares (DVS)

La DVS muestra que cualquier matriz $C$ de tama√±o $n \times p$ y rango $r$ puede factorizarse de la siguiente forma:

$$
C = V L U'
$$

Donde:

-   $L$: matriz diagonal $r \times r$ que contiene los **valores propios** de $C$, es decir, $\sqrt{\lambda_j}$, donde $\lambda_j$ es el $j$-√©simo valor propio de $C'C$
-   $U$: matriz $p \times r$ cuyas columnas son los **vectores propios de** $C'C$
-   $V$: matriz $n \times r$ cuyas columnas son los **vectores propios de** $CC'$
-   Ambas matrices $U$ y $V$ son **ortonormales**: $U'U = V'V = I_r$

Luego, cuando aplicamos la DVS a la matriz $X$ centrada y estandarizada, obtenemos que:

$$
X = V L U'
$$

Donde:

-   $L$ contiene $\sqrt{\lambda_\alpha}$, con $\lambda_\alpha$ siendo los valores propios de $X'X$
-   Las columnas de $U$ son los vectores propios de $X'X$
-   Las columnas de $V$ son los vectores propios de $XX'$

Ahora bien, Se puede escribir X en su forma expandida

$$
X \;=\; [\,v_1\; v_2\; \dots\; v_p\,]
\;
\begin{bmatrix}
\sqrt{\lambda_1} & 0 & \cdots & 0\\[4pt]
0 & \sqrt{\lambda_2} & \cdots & 0\\[4pt]
\vdots & \vdots & \ddots & \vdots\\[4pt]
0 & 0 & \cdots & \sqrt{\lambda_p}
\end{bmatrix}
\;
\begin{bmatrix}
u_1'\\[4pt]
u_2'\\[4pt]
\vdots\\[4pt]
u_p'
\end{bmatrix}
$$

Luego, al multiplicar ((V)) por ((L)) tenemos que:

$$ VL =  [ \sqrt{\lambda_1} v_1, \sqrt{\lambda_2} v_2, \dots, \sqrt{\lambda_p} v_p ] $$

y al multiplicar por ((U')):

$$
X \;=\; \big[\,\sqrt{\lambda_1}\,v_1,\; \sqrt{\lambda_2}\,v_2,\; \dots,\; \sqrt{\lambda_p}\,v_p\,\big]
\begin{bmatrix}
u_1'\\[4pt]
u_2'\\[4pt]
\vdots\\[4pt]
u_p'
\end{bmatrix}
$$ $$
X \;=\; \sqrt{\lambda_1}\,v_1 u_1' \;+\; \sqrt{\lambda_2}\,v_2 u_2' \;+\; \cdots \;+\; \sqrt{\lambda_p}\,v_p u_p'
$$

$$
X\;=\; \sum_{\alpha=1}^{p} \sqrt{\lambda_\alpha}\, v_\alpha u_\alpha'
$$ Esto nos muestra que toda la informaci√≥n de la matriz original est√° contenida en sus valores y vectores propios.

Por otro lado, un resultado clave que surge de la DVS es la **equivalencia entre los valores propios de** (X'X) **y** (XX'). Ambos comparten los mismos valores propios no nulos ($\lambda_{\alpha})$. Esto es:

-   Si ($u_{\alpha}$) es un vector propio de (X'X), entonces $\frac{1}{\sqrt{\lambda_\alpha}}\,Xu_\alpha$ es un vector propio de (XX').

-   An√°logamente, si ($v_{\alpha}$) es un vector propio de (XX'), entonces\
    $\frac{1}{\sqrt{\lambda_\alpha}}\,X'v_\alpha$ es un vector propio de (X'X).

Esto implica que **solo es necesario calcular los valores y vectores propios de una de estas matrices** para obtener tambi√©n los de la otra.

# Equivalencia entre el enfoque de direcciones de m√°xima varianza y el de aproximaci√≥n por m√≠nimos cuadrados

Partiendo de la expansi√≥n en valores singulares de la matriz estandarizada $$
\mathbf{Y} = \sum_{\alpha=1}^{p} \sqrt{\lambda_{\alpha}} \mathbf{v}_{\alpha} \mathbf{u}_{\alpha}',
$$ cada entrada de $\mathbf{Y}$ se escribe como $$
y_{ij} = \sum_{\alpha=1}^{p} \sqrt{\lambda_{\alpha}} v_{i\alpha} u_{j\alpha}.
$$ luego se sabe que la aproximaci√≥n de rango $q$ se obtiene truncando esta suma en los primeros $q$ t√©rminos: $$
\hat{y}_{ij} = \sum_{\alpha=1}^{q} \sqrt{\lambda_{\alpha}} v_{i\alpha} u_{j\alpha}.
$$ Entonces, se puede ver que el residuo de la aproximaci√≥n es exactamente: $$
y_{ij} - \hat{y}_{ij} = \sum_{\alpha=q+1}^{p} \sqrt{\lambda_{\alpha}} v_{i\alpha} u_{j\alpha}.
$$ Al tomar sumas de cuadrados sobre todas las observaciones y variables y usar la ortonormalidad de los autovectores, se obtiene que la suma de cuadrados residual queda dada por la suma de los autovalores descartados, esto es; $$
SCR = \sum_{j=1}^{p} \sum_{i=1}^{n} (y_{ij} - \hat{y}_{ij})^2 = \sum_{\alpha=q+1}^{p} \lambda_{\alpha},
$$ mientras que la suma de cuadrados total satisface $SCT = \sum_{\alpha=1}^{p} \lambda_{\alpha}$. De aqu√≠ se puede deduce que minimizar la suma de cuadrados residual equivale a maximizar la varianza explicada por las $q$ primeras componentes; $$
\tau_q = \frac{\sum_{\alpha=1}^{q} \lambda_{\alpha}}{\sum_{\alpha=1}^{p} \lambda_{\alpha}} = 1 - \frac{SCR}{SCT}.
$$ En consecuencia, las direcciones que resuelven el problema de m√°xima varianza y las que resuelven el problema de aproximaci√≥n por m√≠nimos cuadrados coinciden, asi, ambas son los vectores propios de $\mathbf{Y}'\mathbf{Y}$ asociados a los mayores autovalores, entonces las soluciones son equivalentes.

# Varianza de las Componentes Principales

La varianza de cada componente principal puede obtenerse utilizando el Teorema de la Descomposici√≥n Espectral (TDE). Si $Y$ es la matriz de datos centrados y estandarizados, la matriz de correlaciones se define como:

$$ R = \frac{1}{n} Y'Y. $$

El TDE garantiza que esta matriz puede descomponerse como:

$$ R = U \Lambda U', $$

donde:

-   $U$ es una matriz ortogonal cuyas columnas son los **vectores propios estandarizados** de $R$,
-   $\Lambda = \mathrm{diag}(\lambda_1, \lambda_2, \dots, \lambda_p)$ contiene los **valores propios** ordenados de mayor a menor.

Cada valor propio $\lambda_\alpha$ corresponde a la **varianza** del $\alpha$-√©simo componente principal:

$$ \operatorname{var}(z_\alpha) = \lambda_\alpha. $$

A partir de esto, el **porcentaje de varianza explicada** por el $\alpha$-√©simo componente se define como:

$$ \tau_\alpha = \frac{\lambda_\alpha}{\sum_{i=1}^{p} \lambda_i}. $$

Asimismo, la **varianza explicada acumulada** por los primeros $q$ componentes es:

$$ \tau_q = \frac{\sum_{\alpha = 1}^{q} \lambda_\alpha}{\sum_{i = 1}^{p} \lambda_i}. $$

Estas cantidades permiten evaluar cu√°nto de la informaci√≥n original est√° siendo representada por los componentes retenidos. En general, se seleccionan los primeros componentes que explican un porcentaje adecuado de la varianza total, lo cual asegura una representaci√≥n eficiente del conjunto de datos en menos dimensiones.

# Elementos para la interpretaci√≥n de un ACP

La interpretaci√≥n de un An√°lisis de Componentes Principales (ACP) requiere integrar diversos elementos que describen la estructura interna de los datos, la contribuci√≥n de las variables y la organizaci√≥n de los objetos en el espacio factorial. Aunque cada indicador puede analizarse por separado, la interpretaci√≥n final debe ser conjunta para obtener una lectura coherente del fen√≥meno estudiado. A continuaci√≥n, se presentan los principales elementos utilizados en la interpretaci√≥n de un ACP

### Calidad de la representaci√≥n

La calidad de la representaci√≥n eval√∫a qu√© tanto los componentes principales logran resumir la informaci√≥n contenida en las variables originales. Se basa en la varianza explicada acumulada, que indica el porcentaje de variabilidad capturada por los primeros componentes.

Los criterios m√°s comunes para decidir cu√°ntos componentes conservar son:

-   Alcanzar un porcentaje deseado de varianza acumulada (70%‚Äì90%).
-   Conservar los componentes con autovalor mayor que 1 (regla de Kaiser).
-   Analizar el gr√°fico de sedimentaci√≥n (*scree plot*).

Una buena selecci√≥n asegura una reducci√≥n de la dimensionalidad sin p√©rdida significativa de informaci√≥n.

### Correlaciones variable‚Äìfactor

Las correlaciones variable‚Äìfactor, tambi√©n llamadas cargas\* o \*loadings, indican el grado en que cada variable original est√° asociada a un componente principal. Estas correlaciones permiten evaluar cu√°nto aporta una variable a la construcci√≥n de un componente y qu√© tan bien queda representada en el espacio reducido.

Para la variable $Y_j$ y el componente $\alpha$, la correlaci√≥n variable‚Äìfactor est√° dada por:

$$
w_{j\alpha} = \sqrt{\lambda_\alpha}\, u_{j\alpha},
$$

donde $u_{j\alpha}$ es el elemento del vector propio correspondiente al componente $\alpha$, y $\lambda_\alpha$ es su autovalor. Esta cantidad corresponde exactamente a la correlaci√≥n entre la variable original y el componente principal.

Valores altos de $w_{j\alpha}$ indican que la variable est√° bien explicada por ese componente y contribuye de manera importante a su interpretaci√≥n.

### T√≠pificaci√≥n de los factores por las variables

Para interpretar los factores (componentes) se utilizan diversos indicadores que relacionan las variables originales con los ejes factoriales obtenidos en el ACP.

#### Coordenadas de las variables (scores)

Las coordenadas de las variables se obtienen como $w_\alpha = Y' v_\alpha$, donde $v_\alpha$ es el vector propio asociado al componente $\alpha$. Estas coordenadas coinciden con las correlaciones variable‚Äìfactor, por lo que permiten identificar qu√© variables influyen m√°s en cada componente principal.

#### Contribuciones de las variables

La contribuci√≥n de la variable $j$ al componente $\alpha$ indica qu√© proporci√≥n de la varianza del componente es explicada por esa variable. Se calcula mediante:

$$
\text{contribuci√≥n}_{j\alpha} = \frac{z_{\alpha j}^2}{\lambda_\alpha},
$$

donde $z_{\alpha j}$ es la coordenada de la variable en el componente y $\lambda_\alpha$ su valor propio. Contribuciones altas indican que la variable participa de manera importante en la construcci√≥n del componente.

#### Cosenos cuadrados (cos¬≤)

Los cosenos cuadrados representan la cantidad de varianza de la variable que es explicada por el componente $\alpha$. Corresponden a la correlaci√≥n variable‚Äìfactor elevada al cuadrado. Si $u_{j\alpha}$ es la carga del vector propio, entonces:

$$
\cos^2(\theta_{j,\alpha}) = \lambda_\alpha u_{j\alpha}^2.
$$

Valores grandes de $\cos^2$ indican que la variable est√° bien representada por el componente y que su posici√≥n en el plano factorial es confiable.

### Planos factoriales

Los planos factoriales representan variables y/u objetos mediante pares de componentes principales. Las variables aparecen como vectores desde el origen y los objetos como puntos. La interpretaci√≥n de estos planos permite identificar asociaciones entre variables, agrupamientos entre objetos y la calidad de su representaci√≥n.

A partir de la posici√≥n de las variables en el plano, es posible analizar su relaci√≥n con los componentes, identificar similitudes o asociaciones entre ellas y evaluar su representaci√≥n mediante las coordenadas, contribuciones y cosenos cuadrados. La interpretaci√≥n de los planos factoriales complementa la interpretaci√≥n de los factores y facilita la comprensi√≥n de la estructura multivariante de los datos.

### Distancia entre variables

La distancia entre dos variables en el ACP puede interpretarse mediante el coseno del √°ngulo que forman sus vectores en el espacio factorial. Si $\theta_{j j'}$ es el √°ngulo entre las variables $Y_j$ y $Y_{j'}$, la distancia euclidiana entre ellas puede expresarse como:

$$
d^2(Y_j, Y_{j'}) = 2 \, (1 - \cos(\theta_{j j'})).
$$

Esto permite interpretar la relaci√≥n entre las variables en funci√≥n de su correlaci√≥n:

-   Si $\cos(\theta_{j j'}) \to 1$ (variables muy correlacionadas), entonces $d(Y_j, Y_{j'}) \to 0$.
-   Si $\cos(\theta_{j j'}) \to 0$ (variables no correlacionadas), entonces $d(Y_j, Y_{j'}) \to \sqrt{2}$.
-   Si $\cos(\theta_{j j'}) \to -1$ (variables inversamente correlacionadas), entonces $d(Y_j, Y_{j'}) \to 2$.

Cuando el ACP se realiza a partir de la matriz de covarianzas, la distancia entre las variables $X_j$ y $X_{j'}$ se expresa como:

$$
d^2(X_j, X_{j'}) = s_j + s_{j'} - 2 s_{j j'},
$$

donde $s_j$ y $s_{j'}$ son las varianzas de las variables y $s_{j j'}$ su covarianza. En ambos casos, distancias peque√±as indican variables similares, mientras que distancias grandes reflejan relaciones d√©biles o inversas entre ellas.

### Tipificaci√≥n de los objetos

Adem√°s de interpretar las variables, el ACP permite analizar las posiciones de los objetos u observaciones (como pa√≠ses, universidades, empresas o individuos) en el espacio factorial. A partir de las coordenadas de los objetos en los componentes principales, as√≠ como de sus contribuciones y cosenos cuadrados, es posible identificar patrones, agrupamientos o tendencias presentes en los datos.

Este an√°lisis permite asociar caracter√≠sticas a los objetos seg√∫n su ubicaci√≥n en los planos factoriales y seg√∫n la influencia que ejercen las variables originales en cada componente. De esta manera, la tipificaci√≥n de los objetos complementa la interpretaci√≥n global del ACP al revelar similitudes y diferencias entre las observaciones.

### Distancia entre objetos

La distancia entre objetos en los planos factoriales del ACP permite interpretar la similitud o diferencia entre las observaciones cuando estas no son an√≥nimas (por ejemplo, ciudades, regiones, universidades, empresas o individuos). Su an√°lisis es an√°logo al utilizado para las variables y se basa en los mismos indicadores: coordenadas, contribuciones y cosenos cuadrados.

En la interpretaci√≥n gr√°fica es importante considerar que:

-   Si dos objetos aparecen cercaen cualquier parte del plano factorial, significa que comparten caracter√≠sticas similares y, por tanto, presentan perfiles parecidos.
-   Si dos objetos se encuentran lejos en el plano, esto indica que difieren notablemente en las variables analizadas.
-   Si los objetos se ubican en cuadrantes opuestos o muestran coordenadas con signos contrarios en los factores, esto sugiere que presentan caracter√≠sticas opuestas y, por lo tanto, representan perfiles antag√≥nicos.

De esta forma, la distancia entre objetos ayuda a identificar agrupamientos, contrastes y patrones relevantes dentro del conjunto de observaciones.

### Relaciones entre objetos y variables: biplots

Un biplot es una representaci√≥n gr√°fica simult√°nea de objetos y variables sobre el mismo plano factorial. Este tipo de gr√°fico permite analizar de forma conjunta c√≥mo se relacionan las observaciones con las variables originales del estudio.

La interpretaci√≥n se basa en la proximidad entre los objetos y los vectores que representan a las variables:

-   Los objetos situados cerca de la punta de un vector presentan valores altos en esa variable.
-   Los objetos alejados del vector o en direcci√≥n opuesta presentan valores bajos o contrarios.
-   La relaci√≥n se eval√∫a mediante los mismos indicadores utilizados previamente: coordenadas, contribuciones y cosenos cuadrados.

En conjunto, los biplots permiten identificar qu√© variables explican a cada grupo de objetos y facilitan la interpretaci√≥n de la estructura global del ACP.

### Variables suplementarias

Las variables suplementarias no intervienen en el c√°lculo del ACP, pero se proyectan sobre los componentes obtenidos con las variables activas. Su interpretaci√≥n se basa en su proximidad con las variables activas en el plano factorial.

### Objetos suplementarios

Los objetos suplementarios son observaciones que tampoco participan en la construcci√≥n del ACP. Su proyecci√≥n en el espacio factorial permite evaluar su similitud con los objetos activos sin alterar el an√°lisis principal.
